# AnÃ¡lise: CompressÃ£o de Contexto nas SessÃµes

## ğŸ”„ QUANDO Acontece

**A compressÃ£o Ã© executada ANTES de CADA chamada ao LLM.**

Toda vez que vocÃª envia uma mensagem:
1. Sistema carrega todo o histÃ³rico do banco de dados
2. **CompressÃ£o de contexto Ã© aplicada** â† acontece aqui
3. Prompt caching Ã© aplicado
4. Chamada ao LLM Ã© feita
5. Agente comeÃ§a a responder

## â±ï¸ FREQUÃŠNCIA

**100% das vezes** que o agente vai responder algo.

- Nova mensagem do usuÃ¡rio? â†’ CompressÃ£o
- Agente executou uma ferramenta e vai processar o resultado? â†’ CompressÃ£o
- Auto-continue (agente continua pensando)? â†’ CompressÃ£o

NÃ£o tem exceÃ§Ã£o. Sempre roda.

## ğŸ­ ESTADO DO Agente

**O agente NÃƒO estÃ¡ "running" ainda durante a compressÃ£o.**

A compressÃ£o acontece na fase de preparaÃ§Ã£o, antes do status "running":

```
Timeline:
â”œâ”€ VocÃª envia mensagem
â”œâ”€ [Carregando histÃ³rico do banco...]
â”œâ”€ [COMPRESSÃƒO DE CONTEXTO] â† vocÃª estÃ¡ aqui
â”œâ”€ [Aplicando cache...]
â”œâ”€ Status muda para "running" â† sÃ³ agora
â””â”€ Agente comeÃ§a a responder
```

Durante a compressÃ£o, o agente estÃ¡ em estado de "preparaÃ§Ã£o" - invisÃ­vel ao usuÃ¡rio em conversas normais.

## ğŸ”§ COMO Funciona

### EstratÃ©gia: Remove/trunca mensagens antigas

**Ordem de processÃ£o:**

1. **Remove metadados desnecessÃ¡rios** de tool executions
2. **Comprime tool results antigos** (mas mantÃ©m o mais recente intacto)
3. **Comprime mensagens antigas do usuÃ¡rio** (mas mantÃ©m a mais recente intacta)
4. **Comprime respostas antigas do assistente** (mas mantÃ©m a mais recente intacta)

### O que significa "comprimir"?

**Para mensagens antigas:**
```
ANTES:
[ConteÃºdo longo com 5000 tokens...]

DEPOIS:
[Primeiros 3000 caracteres]... (truncated)
Use expand-message tool with message_id "xyz" to see full content
```

**Para a mensagem mais recente:**
```
MantÃ©m inÃ­cio + fim, remove o meio se necessÃ¡rio
```

## ğŸ“ Limites por Modelo

Cada modelo tem um limite diferente baseado no context window:

| Modelo | Context Window | Limite Usado | Margem Reservada |
|--------|----------------|--------------|------------------|
| Claude Sonnet 4.5 | 200k | 168k tokens | 32k para resposta |
| GPT-5 Codex | 400k | 336k tokens | 64k para resposta |
| Gemini 2.5 Pro | 2M | 1.7M tokens | 300k para resposta |

**Se ultrapassar o limite mesmo apÃ³s compressÃ£o:**
- Tenta comprimir mais agressivamente (atÃ© 5 tentativas)
- Se ainda nÃ£o couber: comeÃ§a a **remover mensagens do meio** da conversa
- Preserva sempre: inÃ­cio da conversa + mensagens recentes

## âš¡ Impacto de Performance

**Em conversas normais (< 50 mensagens):**
- ImperceptÃ­vel (< 150ms)

**Em conversas longas (200-500 mensagens):**
- Pode adicionar 400ms-1s de latÃªncia
- VocÃª percebe um delay mÃ­nimo antes do agente comeÃ§ar a responder

**Em conversas muito longas (500+ mensagens):**
- 1-2.5 segundos de overhead
- NotÃ¡vel, mas necessÃ¡rio para caber no contexto

## ğŸ’° Economia

Com compressÃ£o + caching juntos:
- **70-90% reduÃ§Ã£o de custo** em conversas longas
- **70-90% reduÃ§Ã£o de latÃªncia LLM** (tempo que o modelo leva para processar)

## ğŸ” Detalhes TÃ©cnicos

### ImplementaÃ§Ã£o

**LocalizaÃ§Ã£o do cÃ³digo:**
- `backend/core/agentpress/context_manager.py` - LÃ³gica de compressÃ£o
- `backend/core/agentpress/thread_manager.py:304-317` - AtivaÃ§Ã£o automÃ¡tica

**ConfiguraÃ§Ã£o:**
```python
# backend/core/agentpress/thread_manager.py
ENABLE_CONTEXT_MANAGER = True   # CompressÃ£o SEMPRE ativa
ENABLE_PROMPT_CACHING = True    # Cache SEMPRE ativo
```

### Thresholds de CompressÃ£o

**Threshold inicial:** 1000 tokens por mensagem

Se uma mensagem ultrapassar 1000 tokens e nÃ£o for a mais recente:
- Ã‰ comprimida para ~3000 caracteres
- Adiciona referÃªncia para `expand-message` tool

**CompressÃ£o recursiva:**
Se mesmo apÃ³s compressÃ£o inicial ainda ultrapassar o limite:
1. IteraÃ§Ã£o 1: Threshold = 1000 tokens
2. IteraÃ§Ã£o 2: Threshold = 500 tokens
3. IteraÃ§Ã£o 3: Threshold = 250 tokens
4. IteraÃ§Ã£o 4: Threshold = 125 tokens
5. IteraÃ§Ã£o 5: Threshold = 62 tokens

### Fallback: RemoÃ§Ã£o de Mensagens

Se apÃ³s 5 iteraÃ§Ãµes ainda nÃ£o couber, remove mensagens do meio:

**EstratÃ©gia Middle-Out:**
```
ANTES (100 mensagens):
[1, 2, 3, ..., 48, 49, 50, 51, 52, ..., 98, 99, 100]

DEPOIS (60 mensagens):
[1, 2, 3, ..., 28, 29, 30] + [71, 72, 73, ..., 98, 99, 100]
         â†‘ 30 primeiras          â†‘ 30 Ãºltimas
```

**ParÃ¢metros:**
- `removal_batch_size`: 10 mensagens por vez
- `min_messages_to_keep`: MÃ­nimo de 10 mensagens

## ğŸ“Š Prompt Caching (Segunda Camada)

ApÃ³s compressÃ£o, o sistema aplica prompt caching da Anthropic:

**Blocos de cache:**
1. **Bloco 1**: System prompt (se â‰¥1024 tokens)
2. **Blocos 2-4**: Chunks da conversa (dinÃ¢micos)

**Thresholds dinÃ¢micos por estÃ¡gio:**

| EstÃ¡gio | Mensagens | Multiplier | Threshold (Claude Sonnet 4) |
|---------|-----------|------------|---------------------------|
| Early | â‰¤20 | 0.3x | 7.5k tokens |
| Growing | 21-100 | 0.6x | 15k tokens |
| Mature | 101-500 | 1.0x | 25k tokens |
| Very Long | 500+ | 1.8x | 45k tokens |

**Economia de cache:**
- Cache write: 1.25x custo base
- Cache hit: 0.1x custo base (90% economia!)
- Break-even: 2-3 reutilizaÃ§Ãµes

## ğŸ”§ Monitoramento

### Logs DisponÃ­veis

```bash
# Backend logs - compressÃ£o
tail -f backend/logs/app.log | grep "Context compression"

# Backend logs - caching
tail -f backend/logs/app.log | grep "ğŸ”¥ Block"
```

**Exemplo de log:**
```
Context compression: 150000 -> 85000 tokens
ğŸ”¥ Block 1: Cached chunk (25000 tokens, 45 messages)
ğŸ”¥ Block 2: Cached chunk (30000 tokens, 60 messages)
ğŸ¯ Total cache blocks used: 2/4
```

## ğŸ“ Resumo Executivo

**Resumo em 1 frase:**
A compressÃ£o roda automaticamente antes de toda chamada LLM, durante a fase de preparaÃ§Ã£o (antes do "running"), e funciona truncando/removendo mensagens antigas para caber no limite do modelo.

**Pontos-chave:**
- âœ… Executada em 100% das interaÃ§Ãµes
- âœ… InvisÃ­vel ao usuÃ¡rio em conversas normais
- âœ… Preserva sempre as mensagens mais recentes intactas
- âœ… Economiza 70-90% de custo + latÃªncia
- âœ… EscalÃ¡vel para threads de 1000+ mensagens
- âœ… ConfiguraÃ§Ã£o zero - funciona automaticamente
